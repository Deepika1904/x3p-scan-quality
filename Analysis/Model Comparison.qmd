---
title: "Scan Quality Assessor: Model Comparison"
author: "Heike Hofmann, Craig Orman, Naga Vempati"
output: html_document
format:
  html:
    toc: true
    toc-location: right
---
```{r, include = FALSE, warning=FALSE}
if (!require(tidyverse)) install.packages('tidyverse')
if (!require(ggplot2)) install.packages('ggplot2')
if (!require(randomForest)) install.packages('randomForest')
if (!require(irr)) install.packages('irr')
if (!require(corrplot)) install.packages('corrplot')
if (!require(MASS)) install.packages('MASS')
if (!require(RColorBrewer)) install.packages('RColorBrewer')
if (!require(yardstick)) install.packages('yardstick')
library(tidyverse)
library(ggplot2)
library(randomForest)
library(irr)
library(corrplot)
library(MASS)
library(RColorBrewer)
library(yardstick)
set.seed(10247693)
## Load Functions - eventually we would like to just load a library
theme_set(theme_bw())
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

source("../R/comparison.R")

colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}

hh <- function(x) {
  colorize(x, "darkorange")
}

nv <- function(x) {
  colorize(x, "chartreuse3")
}

co <- function(x) {
  colorize(x, "red")
}
```

## Introduction
This document will give well commented exact technical details to show the comparison of various models we tested and compared to determine the best candidate for use in a production environment.

We will investigate the ability of our features to predict the quality of a scan using a variety of methods. It should be noted that given our current classification system Good > Tiny Problems > Problematic > Bad > Yikes, we will consider that any scan worse than Tiny Problems should be re-scanned by the user. However, this is a tool meant to aid a user in deciding and is not to be taken as a sole source of truth. 

Models will be broken into two classification types, ordinal and binary. In the ordinal models, we will try to accurately predict which of the five categories a scan falls into. As quality is a subjective rating, we will focus more on being within one step of the assigned rating than going for perfect agree, although perfect agreement would be optimal. In the binary setting, we will group the scans as Good (1) and Bad (0) as binary classifications are often easier. Good (1) scans will be scans currently labelled as Good or Tiny Problems, with Bad (0) scans being Problematic, Bad, and Yikes. 

- `r co("Is there a more mathy and rigorous reason we chose these models? They are pretty much just my go-tos, and the only ones I know for categorical")` 

The models we will train are logistic regressions, and random forests. These models have often proved o be sufficiently explainable for forensic purposes, as well as statistically sound in many environments. 

Data handling and type setting

```{r}
full.data <- read.csv2("../data/std_and_cropped_data_12_20_2022.csv", sep=",")
full.data <- full.data %>% mutate(
  Quality = factor(Quality, levels = c("Good", "Tiny Problems", "Problematic", "Bad", "Yikes"), ordered = TRUE),
  Problem = factor(Problem, levels = c("Good", "Damage", "Holes", "Feathering", "Rotation-Staging"), ordered = FALSE),
  # This is the features ran against the full image
  assess_percentile_na_proportion = as.numeric(assess_percentile_na_proportion),
  assess_col_na = as.numeric(assess_col_na),
  extract_na = as.numeric(extract_na),
  assess_middle_na_proportion = as.numeric(assess_middle_na_proportion),
  assess_rotation = as.numeric(assess_rotation),
  assess_bottomempty = as.numeric(assess_bottomempty),
  assess_median_na_proportion = as.numeric(assess_median_na_proportion),
  # This is the features ran against the cropped image
  assess_percentile_na_proportion_cropped = as.numeric(assess_percentile_na_proportion_cropped),
  assess_col_na_cropped = as.numeric(assess_col_na_cropped),
  extract_na_cropped = as.numeric(extract_na_cropped),
  assess_bottomempty_cropped = as.numeric(assess_bottomempty_cropped),
  assess_median_na_proportion_cropped = as.numeric(assess_median_na_proportion_cropped)
)


followupScans <- data.frame()
```

## Feature Analysis

In this investigation, we are going to look at the features calculated against the full image. This investigation will tell us more about the predictive power of each feature, and tell us how different the quality categories are from each other.

```{r}
standard.data <- full.data[,1:12 & 18:19]
table(standard.data[,4:5])
```

### Assess Percentile NA Proportion 

```{r}
summary(standard.data$assess_percentile_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_percentile_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Percentile NA Proportion by Quality")

summary(glm(Quality ~ assess_percentile_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Percentile NA Proportion increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence, with reservation, for this being a useful feature in explaining the quality.

### Assess Col NA

```{r}
summary(standard.data$assess_col_na)
ggplot(standard.data, aes(x=Quality, y=assess_col_na)) +
  geom_boxplot() +
  ggtitle("Assess Col NA by Quality")
summary(glm(Quality ~ assess_col_na,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Col NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart, with the exception of "Good" and "Tiny Problems" which are very similar. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Extract NA

```{r}
summary(standard.data$extract_na)
ggplot(standard.data, aes(x=Quality, y=extract_na)) +
  geom_boxplot() +
  ggtitle("Extract NA by Quality")
summary(glm(Quality ~ extract_na,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Extract NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Middle NA Proportion

```{r}
summary(standard.data$assess_middle_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_middle_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Middle NA Proportion by Quality")
summary(glm(Quality ~ assess_middle_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Extract NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Rotation

```{r}
summary(standard.data$assess_rotation)
ggplot(standard.data, aes(x=Quality, y=assess_rotation)) +
  geom_boxplot() +
  ggtitle("Assess Rotation by Quality")
summary(glm(Quality ~ assess_rotation,
                     data=standard.data, family="binomial"))
```

In the boxplot, we see that there is no significant visual difference in median or IQR of Assess Rotation when grouped by Quality. The logistic regression shows no evidence that this feature can help explain the quality of an image. This feature was primarily intended for use in predicting a particular problem that occurs in scans. 

### Assess Bottomempty

```{r}
summary(standard.data$assess_bottomempty)
ggplot(standard.data, aes(x=Quality, y=assess_bottomempty)) +
  geom_boxplot() +
  ggtitle("Assess Bottomempty by Quality")
summary(glm(Quality ~ assess_bottomempty,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Bottomempty increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. There is an observation of note that the Yikes category has a particularly large IQR. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Median NA Proportion

```{r}
summary(standard.data$assess_median_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_median_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Median NA Proportion by Quality")
summary(glm(Quality ~ assess_median_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Median NA Proportion increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The "Bad" and "Yikes" categories have very similar IQR and Medians. The logistic regression also shows that there is overwhelming evidence, with reservation, for this being a useful feature in explaining the quality.

### Cross Correlations

XXX include https://heike.github.io/ggpcp/ and a scatterplot of the data
```{r}
correlations <- cor(standard.data[,6:12])
corrplot(correlations, method = "shade")
```

There is significant correlation between most of the variables except assess rotation. 

## Model Analysis

**Categorical Predictions:**

For the purposes of this analysis, consider that the scanner using this model to decide if a scan needs to be re-done will be instructed that Good and Tiny Problems predictions likely don't need to be re-done. However, Problematic, Bad, and Yikes scans have a high likelihood of needing to be redone.

**Quantitative Predictions:**

    --cutoff is....

```{r}
full.data = full.data %>% mutate(
  GoodScan = Quality %in% c("Good", "Tiny Problems") %>% factor()
)

sample <- sample(c(TRUE, FALSE), nrow(standard.data), replace=TRUE, prob=c(0.75,0.25))
train  <- standard.data[sample, ]
test   <- standard.data[!sample, ]

train$followup <- FALSE
train$num.Quality <- 0
train$num.Quality[train$Quality == "Good"] <- 1
train$num.Quality[train$Quality == "Tiny Problems"] <- 1
train$num.Quality <- factor(train$num.Quality, levels = c(0, 1))

test$followup <- FALSE
test$num.Quality <- 0
test$num.Quality[test$Quality == "Good"] <- 1
test$num.Quality[test$Quality == "Tiny Problems"] <- 1
test$num.Quality <- factor(test$num.Quality, levels = c(0, 1))

print("Original")
table(standard.data[,4:5])
print("Train")
table(train[,4:5])
print("Test")
table(test[,4:5])

print("Train Numeric")
table(train$num.Quality)
print("Train Numeric")
table(train$num.Quality, train$Problem)

print("Test Numeric")
table(test$num.Quality)
print("Test Numeric")
table(test$num.Quality, test$Problem)
```


### Ordinal Logistic Regression
```{r}
ord.Model <- polr(Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty,
                     data=train, method="logistic")

summary(ord.Model)

test$Predictions <- predict(ord.Model, test)
# True values on the top, Predictions on the left
table(test$Predictions, test$Quality)

```

 The model did not predict a single Good scan in all 459 scans of the test data. It did however, keep all of the Good scans in the Tiny Problems category. The model got 254/257 tiny problems accurately predicted, with one concern that a Tiny Problems scan got labelled as Yikes. So what should have been an acceptable scan got labelled as completely terrible. The Problematic category has some significant spread, concerningly, 59% of the Problematic scans, which is out cutoff of non-acceptability got labelled as Tiny Problems. The Problematic category is consistently incorrectly labelled as better than it is. The Bad category is correctly labelled 55% of the time, and is consistently labelled in the non-acceptable category, with only 9 out of 51 scans being placed wrong. The Yikes category was consistently labelled as bad or yikes, and therefore is entirely in the non-acceptable category, which is what we are looking for. 

### Numerical regression
  - (Good/Tiny = 1, Problematic/Bad/Yikes = 0)

```{r}
num.logit.model <- glm(num.Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty,
                     data=train, family="binomial")

summary(num.logit.model)

test$num.predictions <- predict(num.logit.model, test, type="response")

ggplot(test, aes(x=num.Quality, y=num.predictions)) +
  geom_boxplot() +
  xlab("True Quality") +
  ylab("Probability of good scan") +
  ggtitle("Numerical Logistic Prediction Test Data")

XXX Figure out followup scans

test  <- test %>% 
  mutate(followup=GoodScan=="FALSE" & num.predictions>50)
 test %>% 
   ggplot(aes(x = assess_bottomempty_cropped, y = GoodScan, color = followup)) + 
   geom_jitter() +
   scale_colour_manual(values=c("grey50", "darkorange"))
```


### Ordinal/categorical random forest

Metrics 
    -McNemars
    -kappas
    -various agreement outputs and stuff.
    

```{r}
ord.forest.model <- randomForest(Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty, data = train,
                           importance = TRUE)

test$Prediction <- predict(ord.forest.model, test)
print("Test data")
table(test$Quality, test$Prediction)
```

### Numerical Random Forest
  - (Good/Tiny = 1, Problematic/Bad/Yikes = 0)
  - (Good = 1, Yikes = 0, drop others)

```{r}
num.RF.model <- randomForest(num.Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty, data = train,
                           importance = TRUE)

test$num.predictions <- predict(num.RF.model, test, type = 'prob')[,2]

ggplot(test, aes(x=num.Quality, y=num.predictions)) +
  geom_boxplot() +
  xlab("True Quality") +
  ylab("Probability of good scan") +
  ggtitle("Numerical Random Forest Prediction Test Data")
```

## Cropped Feature Analysis

### Extract NA Cropped

#### Which of the features is better for discriminating between good and bad scans?

```{r echo=FALSE}
correlation <- cor(full.data$extract_na, full.data$extract_na_cropped)

res <- comparison(data.frame(full.data$extract_na, full.data$extract_na_cropped, full.data$Quality), feature = "Extract NA")

res$scatterplot + coord_equal()

res$boxplot

res$roc_curve

print(paste("Extract NA. Correlation: ", round(correlation, 3), "Full AUC:", round(res$roc_auc$Full_AUC, 3), "Cropped AUC: ", round(res$roc_auc$Cropped_AUC, 3)))

knitr::kable(res$summ, caption=attr(res$summ, "title"))

```
#### Should we use features from just one type of scan or both?

```{r, echo=FALSE}
# logistic regression in the two features
logistic_base <- glm(GoodScan~extract_na+extract_na_cropped, data = full.data, family = binomial())
summary(logistic_base)

# extract_na_cropped is the better single predictor. 
full.data %>% pivot_longer(starts_with("extract_na"), names_to="Scan") %>% 
  ggplot(aes(x = value, fill=GoodScan, color=GoodScan)) +
  geom_density(alpha=0.8) +
  scale_fill_manual(values=col_scans_light) +
  scale_colour_manual(values=col_scans_dark) +
  facet_grid(.~Scan)

```


#### Conclusion for Extract NA

The values for feature `extract_NA` are highly correlated between the cropped and the full scan. 

Using good and scans with only tiny problems as overall 'good' scans, the feature applied to cropped scans has an increased accuracy compared to the feature values from the full scan. 

We might want to follow up on the orange colored scans:

```{r echo=FALSE, fig.height=3}
full.data  <- full.data %>% 
  mutate(followup=GoodScan=="TRUE" & extract_na_cropped>15)
full.data %>% 
  ggplot(aes(x = extract_na_cropped, y = GoodScan, color = followup)) + 
  geom_jitter() +
  scale_colour_manual(values=c("grey50", "darkorange"))


```
```{r}

full.data$LAPD_id[full.data$followup]

followupScans <- rbind(followupScans, full.data[full.data$followup == TRUE,])
# All followups for extract_na are mislabelled scans. They are all labelled as tiny problems but should be problematic or worse.
```

```{r, include = FALSE, eval = FALSE}
library(x3ptools)
# /media/Raven/LAPD
f1 <- x3p_read("/media/Raven/LAPD/FAU 263/Bullet A/LAPD - 263 - Bullet A - Land 4 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU263-BA-L4 is labelled tiny-problems but should be labelled Problematic or worse

f2 <- x3p_read("/media/Raven/LAPD/FAU 263/Bullet C/LAPD - 263 - Bullet C - Land 1 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU263-BC-L1 is labelled tiny-problems but should be labelled Problematic or worse

f3 <- x3p_read("/media/Raven/LAPD/FAU 263/Bullet C/LAPD - 263 - Bullet C - Land 3 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU263-BC-L3 is labelled tiny-problems but should be labelled Problematic or worse

f4 <- x3p_read("/media/Raven/LAPD/FAU 287/Bullet C/LAPD - 287 - Bullet C - Land 5 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU287-BC-L5 is labelled tiny-problems but should be labelled Problematic or worse

f5 <- x3p_read("/media/Raven/LAPD/FAU 154/Bullet D/LAPD - 154 - Bullet D - Land 2 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Carley McConnell.x3p")
# FAU154-BD-L2 is labelled tiny-problems but should be labelled Problematic or worse

f6 <- x3p_read("/media/Raven/LAPD/FAU 277/Bullet A/LAPD - 277 - Bullet A - Land 4 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU277-BA-L4 is labelled tiny-problems but should be labelled Problematic or worse

f7 <- x3p_read("/media/Raven/LAPD/FAU 286/Bullet A/LAPD - 286 - Bullet A - Land 5 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU286-BA-L5 is labelled tiny-problems but should be labelled Problematic or worse
```


### Assess Bottomempty Cropped

#### Which of the features is better for discriminating between good and bad scans?

```{r echo=FALSE}
correlation <- cor(full.data$assess_bottomempty, full.data$assess_bottomempty_cropped)

res <- comparison(data.frame(full.data$assess_bottomempty, full.data$assess_bottomempty_cropped, full.data$Quality), feature = "Assess Bottomempty")
res$scatterplot + coord_equal()

res$boxplot

res$roc_curve

print(paste("Assess Bottomempty. Correlation: ", round(correlation, 3), "Full AUC:", round(res$roc_auc$Full_AUC, 3), "Cropped AUC: ", round(res$roc_auc$Cropped_AUC, 3)))

knitr::kable(res$summ, caption=attr(res$summ, "title"))

```
#### Should we use features from just one type of scan or both?

```{r, echo=FALSE}
# logistic regression in the two features
logistic_base <- glm(GoodScan~assess_bottomempty+assess_bottomempty_cropped, data = full.data, family = binomial())
summary(logistic_base)

# assess_bottomempty_cropped is the better single predictor. 
full.data %>% pivot_longer(starts_with("assess_bottomempty"), names_to="Scan") %>% 
  ggplot(aes(x = value, fill=GoodScan, color=GoodScan)) + geom_density(alpha=0.8) + scale_fill_manual(values=col_scans_light) + scale_colour_manual(values=col_scans_dark) +
  facet_grid(.~Scan)
```


#### Conclusion for Assess Bottomempty

The values for feature `assess_bottomempty` are highly correlated between the cropped and the full scan. 

Using good and scans with only tiny problems as overall 'good' scans, the feature applied to cropped scans has an increased accuracy compared to the feature values from the full scan. 

We might want to follow up on the orange colored scans:

```{r echo=FALSE, fig.height=3}
full.data  <- full.data %>% 
  mutate(followup=GoodScan=="TRUE" & assess_bottomempty_cropped>30)
full.data %>% 
  ggplot(aes(x = assess_bottomempty_cropped, y = GoodScan, color = followup)) + 
  geom_jitter() +
  scale_colour_manual(values=c("grey50", "darkorange"))
```
```{r}
full.data$LAPD_id[full.data$followup]

followupScans <- rbind(followupScans, full.data[full.data$followup == TRUE,])
```

```{r, include = FALSE, eval = FALSE}
# All problems are registered as tiny problems
followup <- c("FAU263-BA-L4", "FAU287-BC-L5", "FAU254-BD-L4", "FAU275-BC-L5", "FAU275-BD-L3", "FAU277-BA-L4", "FAU286-BA-L5")
f1 <- x3p_read("/media/Raven/LAPD/FAU 263/Bullet A/LAPD - 263 - Bullet A - Land 4 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f1, main=followup[1])

f2 <- x3p_read("/media/Raven/LAPD/FAU 287/Bullet C/LAPD - 287 - Bullet C - Land 5 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f2, main=followup[2])

f3 <- x3p_read("/media/Raven/LAPD/FAU 254/Bullet D/LAPD - 254 - Bullet D - Land 4 - Sneox2 - 20x - auto light left image +20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f3, main=followup[3])

f4 <- x3p_read("/media/Raven/LAPD/FAU 275/Bullet C/LAPD - 275 - Bullet C - Land 5 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f4, main=followup[4])

f5 <- x3p_read("/media/Raven/LAPD/FAU 275/Bullet D/LAPD - 275 - Bullet D - Land 3 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f5, main=followup[5])

f6 <- x3p_read("/media/Raven/LAPD/FAU 277/Bullet A/LAPD - 277 - Bullet A - Land 4 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f6, main=followup[6])

f7 <- x3p_read("/media/Raven/LAPD/FAU 286/Bullet A/LAPD - 286 - Bullet A - Land 5 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f7, main=followup[7])

```


### Assess Col NA Cropped

#### Which of the features is better for discriminating between good and bad scans?

```{r echo=FALSE}
correlation <- cor(full.data$assess_col_na, full.data$assess_col_na_cropped)

res <- comparison(data.frame(full.data$assess_col_na, full.data$assess_col_na_cropped, full.data$Quality), feature = "Assess Col NA")

res$scatterplot + coord_equal()

res$boxplot

res$roc_curve

print(paste("Assess Col NA Correlation: ", round(correlation, 3), "Full AUC:", round(res$roc_auc$Full_AUC, 3), "Cropped AUC: ", round(res$roc_auc$Cropped_AUC, 3)))

knitr::kable(res$summ, caption=attr(res$summ, "title"))

```
#### Should we use features from just one type of scan or both?

```{r, echo=FALSE}
# logistic regression in the two features
logistic_base <- glm(GoodScan~assess_col_na+assess_col_na_cropped, data = full.data, family = binomial())
summary(logistic_base)

# Both predictors are about the same.
full.data %>% pivot_longer(starts_with("assess_col_na"), names_to="Scan") %>% 
  ggplot(aes(x = value, fill=GoodScan, color=GoodScan)) + geom_density(alpha=0.8) + scale_fill_manual(values=col_scans_light) + scale_colour_manual(values=col_scans_dark) +
  facet_grid(.~Scan)
```


#### Conclusion for Assess Col NA

The values for feature `assess_col_na` are highly correlated between the cropped and the full scan. 

Using good and scans with only tiny problems as overall 'good' scans, the feature applied to cropped scans has an increased accuracy compared to the feature values from the full scan. 

We might want to follow up on the orange colored scans:

```{r echo=FALSE, fig.height=3}
full.data  <- full.data %>% 
  mutate(followup=GoodScan=="TRUE" & assess_col_na_cropped>1.35)
full.data %>% 
  ggplot(aes(x = assess_col_na_cropped, y = GoodScan, color = followup)) + 
  geom_jitter() +
  scale_colour_manual(values=c("grey50", "darkorange"))
```
```{r}
full.data$LAPD_id[full.data$followup]

followupScans <- rbind(followupScans, full.data[full.data$followup == TRUE,])
```

```{r, include = FALSE, eval = FALSE}
followup <- data.frame(LAPD_ID = c("FAU263-BA-L4", "FAU263-BB-L3", "FAU263-BC-L1", "FAU263-BC-L3", "FAU154-BD-L2", "FAU286-BA-L5"), filePAth = c("/media/Raven/LAPD/FAU 263/Bullet A/LAPD - 263 - Bullet A - Land 4 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p", "/media/Raven/LAPD/FAU 263/Bullet B/LAPD - 263 - Bullet B - Land 3 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p", "/media/Raven/LAPD/FAU 263/Bullet C/LAPD - 263 - Bullet C - Land 1 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p", "/media/Raven/LAPD/FAU 263/Bullet C/LAPD - 263 - Bullet C - Land 3 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p", "/media/Raven/LAPD/FAU 154/Bullet D/LAPD - 154 - Bullet D - Land 2 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Carley McConnell.x3p"  , "/media/Raven/LAPD/FAU 286/Bullet A/LAPD - 286 - Bullet A - Land 5 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p"))

for (i in 1:nrow(followup)) {
  f <- x3p_read(followup[i,2])
  image(f, main=followup[i,1])
}
```

### Assess Median NA Proportion Cropped

#### Which of the features is better for discriminating between good and bad scans?

```{r echo=FALSE}
correlation <- cor(full.data$extract_na, full.data$extract_na_cropped)


res <- comparison(data.frame(full.data$assess_median_na_proportion, full.data$assess_median_na_proportion_cropped, full.data$Quality), feature = "Assess median NA proportion")

res$scatterplot + coord_equal()

res$boxplot

res$roc_curve

print(paste("Assess Median NA Proportion. Correlation: ", round(correlation, 3), "Full AUC:", round(res$roc_auc$Full_AUC, 3), "Cropped AUC: ", round(res$roc_auc$Cropped_AUC, 3)))

knitr::kable(res$summ, caption=attr(res$summ, "title"))

```
#### Should we use features from just one type of scan or both?

```{r, echo=FALSE}
# logistic regression in the two features
logistic_base <- glm(GoodScan~assess_median_na_proportion+assess_median_na_proportion_cropped,
                     data = full.data, family = binomial())
summary(logistic_base)

# assess_median_na_proportion is the better single predictor. 
full.data %>% pivot_longer(starts_with("assess_median_na_proportion"), names_to="Scan") %>% 
  ggplot(aes(x = value, fill=GoodScan, color=GoodScan)) + geom_density(alpha=0.8) + scale_fill_manual(values=col_scans_light) + scale_colour_manual(values=col_scans_dark) +
  facet_grid(.~Scan)
```


#### Conclusion for Assess Median NA Proportion

The values for feature `extract_NA` are highly correlated between the cropped and the full scan. 

Using good and scans with only tiny problems as overall 'good' scans, the feature applied to full scans has an increased accuracy compared to the feature values from the cropped scan. 

We might want to follow up on the orange colored scans:

```{r echo=FALSE, fig.height=3}
full.data  <- full.data %>% 
  mutate(followup=GoodScan=="TRUE" & assess_median_na_proportion>0.095)
full.data %>% 
  ggplot(aes(x = assess_median_na_proportion, y = GoodScan, color = followup)) + 
  geom_jitter() +
  scale_colour_manual(values=c("grey50", "darkorange"))
```
```{r}
full.data$LAPD_id[full.data$followup]

followupScans <- rbind(followupScans, full.data[full.data$followup == TRUE,])
```
```{r, include = FALSE, eval = FALSE}
followup <- data.frame(LAPD_ID = c("FAU263-BC-L3", "FAU154-BD-L2", "FAU204-BC-L4"), filePAth = c("/media/Raven/LAPD/FAU 263/Bullet C/LAPD - 263 - Bullet C - Land 3 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p", "/media/Raven/LAPD/FAU 154/Bullet D/LAPD - 154 - Bullet D - Land 2 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Carley McConnell.x3p", "/media/Raven/LAPD/FAU 204/Bullet C/LAPD - 204 - Bullet C - Land 4 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p"))

for (i in 1:nrow(followup)) {
  f <- x3p_read(followup[i,2])
  image(f, main=followup[i,1])
}
# all should be relabbeled, FAU 204, BC, L4 is particularly Yikes looking
```

### Followup Scans

XXX todo: add standard analysis and predicted outliers

## Cropped Model Analysis
