---
title: "Scan Quality Assessor: Model Comparison"
author: "Heike Hofmann, Craig Orman, Naga Vempati"
output: html_document
---
```{r, include = FALSE, warning=FALSE}
if (!require(tidyverse)) install.packages('tidyverse')
if (!require(ggplot2)) install.packages('ggplot2')
if (!require(randomForest)) install.packages('randomForest')
if (!require(irr)) install.packages('irr')
if (!require(corrplot)) install.packages('corrplot')
if (!require(MASS)) install.packages('MASS')
library(tidyverse)
library(ggplot2)
library(randomForest)
library(irr)
library(corrplot)
library(MASS)
set.seed(17)

colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}

hh <- function(x) {
  colorize(x, "darkorange")
}

nv <- function(x) {
  colorize(x, "chartreuse3")
}

co <- function(x) {
  colorize(x, "red")
}
```

## Introduction
This document will give well commented exact technical details to show the comparison of various models we tested and compared to determine the best candidate for use in a production environment.

Data handling and type setting

```{r}
full.data <- read.csv2("../data/std_and_cropped_data_12_20_2022.csv", sep=",")
full.data.typed <- full.data %>% mutate(
  Quality = factor(Quality, levels = c("Good", "Tiny Problems", "Problematic", "Bad", "Yikes"), ordered = TRUE),
  Problem = factor(Problem, levels = c("Good", "Damage", "Holes", "Feathering", "Rotation-Staging"), ordered = FALSE),
  # This is the features ran against the full image
  assess_percentile_na_proportion = as.numeric(assess_percentile_na_proportion),
  assess_col_na = as.numeric(assess_col_na),
  extract_na = as.numeric(extract_na),
  assess_middle_na_proportion = as.numeric(assess_middle_na_proportion),
  assess_rotation = as.numeric(assess_rotation),
  assess_bottomempty = as.numeric(assess_bottomempty),
  assess_median_na_proportion = as.numeric(assess_median_na_proportion),
  # This is the features ran against the cropped image
  assess_percentile_na_proportion_cropped = as.numeric(assess_percentile_na_proportion_cropped),
  assess_col_na_cropped = as.numeric(assess_col_na_cropped),
  extract_na_cropped = as.numeric(extract_na_cropped),
  assess_bottomempty_cropped = as.numeric(assess_bottomempty_cropped),
  assess_median_na_proportion_cropped = as.numeric(assess_median_na_proportion_cropped)
)
```

## Feature Analysis

In this investigation, we are going to look at the features calculated against the full image. This investigation will tell us more about the predictive power of each feature, and tell us how different the quality categories are from each other.

```{r}
standard.data <- full.data.typed[,1:12]
table(standard.data[,4:5])
```

### Assess Percentile NA Proportion 

```{r}
summary(standard.data$assess_percentile_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_percentile_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Percentile NA Proportion by Quality")

summary(glm(Quality ~ assess_percentile_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Percentile NA Proportion increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence, with reservation, for this being a useful feature in explaining the quality.

### Assess Col NA

```{r}
summary(standard.data$assess_col_na)
ggplot(standard.data, aes(x=Quality, y=assess_col_na)) +
  geom_boxplot() +
  ggtitle("Assess Col NA by Quality")
summary(glm(Quality ~ assess_col_na,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Col NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart, with the exception of "Good" and "Tiny Problems" which are very similar. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Extract NA

```{r}
summary(standard.data$extract_na)
ggplot(standard.data, aes(x=Quality, y=extract_na)) +
  geom_boxplot() +
  ggtitle("Extract NA by Quality")
summary(glm(Quality ~ extract_na,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Extract NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Middle NA Proportion

```{r}
summary(standard.data$assess_middle_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_middle_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Middle NA Proportion by Quality")
summary(glm(Quality ~ assess_middle_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Extract NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Rotation

```{r}
summary(standard.data$assess_rotation)
ggplot(standard.data, aes(x=Quality, y=assess_rotation)) +
  geom_boxplot() +
  ggtitle("Assess Rotation by Quality")
summary(glm(Quality ~ assess_rotation,
                     data=standard.data, family="binomial"))
```

In the boxplot, we see that there is no significant visual difference in median or IQR of Assess Rotation when grouped by Quality. The logistic regression shows no evidence that this feature can help explain the quality of an image. 

### Assess Bottomempty

```{r}
summary(standard.data$assess_bottomempty)
ggplot(standard.data, aes(x=Quality, y=assess_bottomempty)) +
  geom_boxplot() +
  ggtitle("Assess Bottomempty by Quality")
summary(glm(Quality ~ assess_bottomempty,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Bottomempty increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. There is an observation of note that the Yikes category has a particularly large IQR. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Median NA Proportion

```{r}
summary(standard.data$assess_median_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_median_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Median NA Proportion by Quality")
summary(glm(Quality ~ assess_median_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Median NA Proportion increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The "Bad" and "Yikes" categories have very similar IQR and Medians. The logistic regression also shows that there is overwhelming evidence, with reservation, for this being a useful feature in explaining the quality.

### Cross Correlations
```{r}
correlations <- cor(standard.data[,6:12])
corrplot(correlations, method = "shade")
```

There is significant correlation between most of the variables except assess rotation. 

## Model Analysis

**Categorical Predictions:**

For the purposes of this analysis, consider that the scanner using this model to decide if a scan needs to be re-done will be instructed that Good and Tiny Problems predictions likely don't need to be re-done. However, Problematic, Bad, and Yikes scans have a high likelihood of needing to be redone.

**Quantitative Predictions:**

    --cutoff is....

```{r}
sample <- sample(c(TRUE, FALSE), nrow(standard.data), replace=TRUE, prob=c(0.75,0.25))
train  <- standard.data[sample, ]
test   <- standard.data[!sample, ]


train$num.Quality <- 0
train$num.Quality[train$Quality == "Good"] <- 1
train$num.Quality[train$Quality == "Tiny Problems"] <- 1
train$num.Quality <- factor(train$num.Quality, levels = c(1, 0))

test$num.Quality <- 0
test$num.Quality[test$Quality == "Good"] <- 1
test$num.Quality[test$Quality == "Tiny Problems"] <- 1
test$num.Quality <- factor(test$num.Quality, levels = c(1, 0))

print("Original")
table(standard.data[,4:5])
print("Train")
table(train[,4:5])
print("Test")
table(test[,4:5])

print("Train Numeric")
table(train$num.Quality)
print("Train Numeric")
table(train$num.Quality, train$Problem)

print("Test Numeric")
table(test$num.Quality)
print("Test Numeric")
table(test$num.Quality, test$Problem)
```


### Ordinal Logistic Regression
```{r}
ord.Model <- polr(Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty,
                     data=train, method="logistic")

summary(ord.Model)

test$Predictions <- predict(ord.Model, test)
# True values on the top, Predictions on the left
table(test$Predictions, test$Quality)
```

 The model did not predict a single Good scan in all 459 scans of the test data. It did however, keep all of the Good scans in the Tiny Problems category. The model got 254/257 tiny problems accurately predicted, with one concern that a Tiny Problems scan got labelled as Yikes. So what should have been an acceptable scan got labelled as completely terrible. The Problematic category has some significant spread, concerningly, 59% of the Problematic scans, which is out cutoff of non-acceptability got labelled as Tiny Problems. The Problematic category is consistently incorrectly labelled as better than it is. The Bad category is correctly labelled 55% of the time, and is consistently labelled in the non-acceptable category, with only 9 out of 51 scans being placed wrong. The Yikes category was consistently labelled as bad or yikes, and therefore is entirely in the non-acceptable category, which is what we are looking for. 

### Numerical regression
  - (Good/Tiny = 1, Problematic/Bad/Yikes = 0)
  - (Good = 1, Yikes = 0, drop others)
  
  - `r co("XXX Something is very wrong here.")` 

```{r}
num.logit.model <- glm(factor(num.Quality) ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty,
                     data=train, family="binomial")

summary(num.logit.model)

test$num.predictions <- predict(num.logit.model, test)

ggplot(test, aes(x=num.Quality, y=num.predictions)) +
  geom_boxplot() +
  xlab("True Quality") +
  ylab("Probability of good scan") +
  ggtitle("Numerical Logistic Prediction Test Data")
```


### Ordinal/categorical random forest

Metrics 
    -McNemars
    -kappas
    -various agreement outputs and stuff.
    

```{r}
ord.forest.model <- randomForest(Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty, data = train,
                           importance = TRUE)

test$Prediction <- predict(ord.forest.model, test)
print("Test data")
table(test$Quality, test$Prediction)
```

### Numerical Random Forest
  - (Good/Tiny = 1, Problematic/Bad/Yikes = 0)
  - (Good = 1, Yikes = 0, drop others)

```{r}
num.RF.model <- randomForest(num.Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty, data = train,
                           importance = TRUE)

test$num.predictions <- predict(num.RF.model, test, type = 'prob')[,2]

ggplot(test, aes(x=num.Quality, y=num.predictions)) +
  geom_boxplot() +
  xlab("True Quality") +
  ylab("Probability of good scan") +
  ggtitle("Numerical Random Forest Prediction Test Data")
```

## Cropped Feature Analysis

## Cropped Model Analysis
