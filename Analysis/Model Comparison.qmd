---
title: "Scan Quality Assessor: Model Comparison"
author: "Heike Hofmann, Craig Orman, Naga Vempati"
output: html_document
---
```{r, warning=FALSE}
if (!require(tidyverse)) install.packages('tidyverse')
if (!require(ggplot2)) install.packages('ggplot2')
if (!require(randomForest)) install.packages('randomForest')
library(tidyverse)
library(ggplot2)
library(randomForest)
```

## Introduction
This document will give well commented exact technical details to show the comparison of various models we tested and compared to determine the best candidate for use in a production environment.

OUTLINE:
Look at quantitative data
Look at feature spread
compare usefulness of each feature
Compare a bunch of models

Data handling and type setting

```{r}
full.data <- read.csv2("../data/std_and_cropped_data_12_20_2022.csv", sep=",")
full.data.typed <- full.data %>% mutate(
  Quality = factor(Quality, levels = c("Good", "Tiny Problems", "Problematic", "Bad", "Yikes"), ordered = TRUE),
  Problem = factor(Problem, levels = c("Good", "Damage", "Holes", "Feathering", "Rotation-Staging"), ordered = FALSE),
  # This is the features ran against the full image
  assess_percentile_na_proportion = as.numeric(assess_percentile_na_proportion),
  assess_col_na = as.numeric(assess_col_na),
  extract_na = as.numeric(extract_na),
  assess_middle_na_proportion = as.numeric(assess_middle_na_proportion),
  assess_rotation = as.numeric(assess_rotation),
  assess_bottomempty = as.numeric(assess_bottomempty),
  assess_median_na_proportion = as.numeric(assess_median_na_proportion),
  # This is the features ran against the cropped image
  assess_percentile_na_proportion_cropped = as.numeric(assess_percentile_na_proportion_cropped),
  assess_col_na_cropped = as.numeric(assess_col_na_cropped),
  extract_na_cropped = as.numeric(extract_na_cropped),
  assess_bottomempty_cropped = as.numeric(assess_bottomempty_cropped),
  assess_median_na_proportion_cropped = as.numeric(assess_median_na_proportion_cropped)
)
```

## Feature Analysis

In this investigation, we are going to look at the features calculated against the full image. This investigation will tell us more about the predictive power of each feature, and tell us how different the quality categories are from each other.

```{r}
standard.data <- full.data.typed[,1:12]
table(standard.data[,4:5])
```

### Assess Percentile NA Proportion 

```{r}
summary(standard.data$assess_percentile_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_percentile_na_proportion)) +
  geom_boxplot() +
  ggtitle("v by Quality")

summary(glm(Quality ~ assess_percentile_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Percentile NA Proportion increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence, with reservation, for this being a useful feature in explaining the quality.

### Assess Col NA

```{r}
summary(standard.data$assess_col_na)
ggplot(standard.data, aes(x=Quality, y=assess_col_na)) +
  geom_boxplot() +
  ggtitle("Assess Col NA by Quality")
summary(glm(Quality ~ assess_col_na,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Col NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart, with the exception of "Good" and "Tiny Problems" which are very similar. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Extract NA

```{r}
summary(standard.data$extract_na)
ggplot(standard.data, aes(x=Quality, y=extract_na)) +
  geom_boxplot() +
  ggtitle("Extract NA by Quality")
summary(glm(Quality ~ extract_na,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Extract NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Middle NA Proportion

```{r}
summary(standard.data$assess_middle_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_middle_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Middle NA Proportion by Quality")
summary(glm(Quality ~ assess_middle_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Extract NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Rotation

```{r}
summary(standard.data$assess_rotation)
ggplot(standard.data, aes(x=Quality, y=assess_rotation)) +
  geom_boxplot() +
  ggtitle("Assess Rotation by Quality")
summary(glm(Quality ~ assess_rotation,
                     data=standard.data, family="binomial"))
```

In the boxplot, we see that there is no significant visual difference in median or IQR of Assess Rotation when grouped by Quality. The logistic regression shows no evidence that this feature can help explain the quality of an image. 

### Assess Bottomempty

```{r}
summary(standard.data$assess_bottomempty)
ggplot(standard.data, aes(x=Quality, y=assess_bottomempty)) +
  geom_boxplot() +
  ggtitle("Assess Bottomempty by Quality")
summary(glm(Quality ~ assess_bottomempty,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Bottomempty increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. There is an observation of note that the Yikes category has a particularly large IQR. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Median NA Proportion

```{r}
summary(standard.data$assess_median_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_median_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Median NA Proportion by Quality")
summary(glm(Quality ~ assess_median_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Median NA Proportion increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The "Bad" and "Yikes" categories have very similar IQR and Medians. The logistic regression also shows that there is overwhelming evidence, with reservation, for this being a useful feature in explaining the quality.

### Cross COrrelations
```{r}
correlations <- cor(standard.data[,6:12])
cor.df <- data.frame(correlations)
cor.df
```

```{r}
library(corrplot)

corrplot(correlations, method = "shade")
```

There is significant correlation between most of the variables except assess rotation. 

## Model Analysis

```{r}
sample <- sample(c(TRUE, FALSE), nrow(standard.data), replace=TRUE, prob=c(0.75,0.25))
train  <- standard.data[sample, ]
test   <- standard.data[!sample, ]

print("Original")
table(standard.data[,4:5])
print("Train")
table(train[,4:5])
print("Test")
table(test[,4:5])

```


### Ordinal Logistic Regression
```{r}
ord.reg.train <- train
ord.reg.test <- test

#summary(glm(Quality ~ assess_percentile_na_proportion + assess_col_na +
#              extract_na + assess_middle_na_proportion + assess_bottomempty +
#              assess_median_na_proportion,
#                     data=standard.data, family="binomial"))

ord.Model <- glm(Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty,
                     data=ord.reg.train, family="binomial")

summary(ord.Model)

ord.predictions <- predict(ord.Model, ord.reg.test)
```


### Numerical regression
  - (Good/Tiny = 1, Problematic/Bad/Yikes = 0)
  - (Good = 1, Yikes = 0, drop others)
  
  - `r co("Need to check out the problem distribution and if we should relabel Tiny Problems as Good for the problem")` 

Modifying the data to fit into a numerical regression.


```{r}
num.reg.train <- train
num.reg.test <- test

num.reg.train$num.Quality <- 0
num.reg.train$num.Quality[num.reg.train$Quality == "Good"] <- 1
num.reg.train$num.Quality[num.reg.train$Quality == "Tiny Problems"] <- 1

num.reg.train$num.Quality <- factor(num.reg.train$num.Quality, levels = c(1, 0))

num.reg.test$num.Quality <- 0
num.reg.test$num.Quality[num.reg.test$Quality == "Good"] <- 1
num.reg.test$num.Quality[num.reg.test$Quality == "Tiny Problems"] <- 1

num.reg.test$num.Quality <- factor(num.reg.test$num.Quality, levels = c(1, 0))

table(num.reg.train$num.Quality)

table(num.reg.train$num.Quality, num.reg.train$Problem)
```

Running the model

```{r}
num.model <- glm(num.Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty,
                     data=num.reg.train, family="binomial")

summary(num.model)

num.predictions <- predict(num.model, num.reg.test)
```
  
### Ordinal/categorical random forest

Metrics 
    -Mncemars
    -kappas
    -various agreement outputs and stuff.
    

```{r}
ord.forest.train <- train
ord.forest.test <- test

ord.forest.model <- randomForest(Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty, data = ord.forest.train,
                           importance = TRUE)

ord.forest.model$confusion

print("Training Data")
ord.forest.test$Prediction <- predict(ord.forest.model, ord.forest.test)

print("Test data")
table(ord.forest.test$Quality, ord.forest.test$Prediction)
```


### Numerical Random Forest
  - (Good/Tiny = 1, Problematic/Bad/Yikes = 0)
  - (Good = 1, Yikes = 0, drop others)

```{r}
num.forest.train <- train
num.forest.test <- test

num.forest.train$num.Quality <- 0
num.forest.train$num.Quality[num.forest.train$Quality == "Good"] <- 1
num.forest.train$num.Quality[num.forest.train$Quality == "Tiny Problems"] <- 1

num.forest.train$num.Quality <- factor(num.forest.train$num.Quality, levels = c(1, 0))

num.forest.test$num.Quality <- 0
num.forest.test$num.Quality[num.forest.test$Quality == "Good"] <- 1
num.forest.test$num.Quality[num.forest.test$Quality == "Tiny Problems"] <- 1

num.forest.test$num.Quality <- factor(num.forest.test$num.Quality, levels = c(1, 0))

table(num.forest.train$num.Quality)

table(num.forest.train$num.Quality, num.forest.train$Problem)
```

```{r}
randomForest(num.Quality ~ assess_percentile_na_proportion + assess_col_na +
              assess_middle_na_proportion + assess_bottomempty, data = num.forest.train,
                           importance = TRUE)
```

## Cropped Feature Analysis

## Cropped Model Analysis
